PSEUDO DISTRIBUTED MODE (SINGLE NODE CLUSTER)

NOTE: The above commands work with ubuntu 14.04,15.10 x64 versions.
Donot run any commands in root login use sudo instead.
Hadoop download link might change depending on the apache server please find the exact version 2.6 URL and download the tar file:

http:/www.apache.org

1.update the repository by following command

$ sudo apt-get update

2.install default jdk by following command

$ sudo apt-get install default-jdk

3.check for the the installation location by following command 

$ java –version

4.install ssh by following command

$ sudo apt-get install ssh

5.install rsync by following command

$ sudo apt-get install rsync

6.generate & copy key by following command 

$ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa

$ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys

7.download the hadoop tar file by going into apache site (This URL might change)

$ wget -c http://mirror.olnevhost.net/pub/apache/hadoop/common/current/hadoop-2.6.0.tar.gz

8.extract the file by following command

$ sudo tar -zxvf hadoop-2.6.0.tar.gz

9.move the file to specific location

$ sudo mv hadoop-2.6.0 /usr/local/hadoop

10.update the java alternatives.

$ update-alternatives --config java

11.put entry in bashrc file

$ sudo gedit ~/.bashrc

          #Hadoop Variables
          export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
          export HADOOP_HOME=/usr/local/hadoop
          export PATH=$PATH:$HADOOP_HOME/bin
          export PATH=$PATH:$HADOOP_HOME/sbin
          export HADOOP_MAPRED_HOME=$HADOOP_HOME
          export HADOOP_COMMON_HOME=$HADOOP_HOME
          export HADOOP_HDFS_HOME=$HADOOP_HOME
          export YARN_HOME=$HADOOP_HOME
          export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
          export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"

12.source for applying the changes to file

$ source ~/.bashrc

13.Navigate to hadoop directory

$ cd /usr/local/hadoop/etc/hadoop

14.Update the files as mentioned below

$ sudo gedit hadoop-env.sh

          #The java implementation to use.
          export JAVA_HOME="/usr/lib/jvm/java-7-openjdk-amd64"

$ sudo gedit core-site.xml

          <configuration>
                  <property>
                      <name>fs.defaultFS</name>
                      <value>hdfs://localhost:9000</value>
                  </property>
          </configuration>

$ sudo gedit yarn-site.xml

          <configuration>
                  <property>
                      <name>yarn.nodemanager.aux-services</name>
                      <value>mapreduce_shuffle</value>
                  </property>
                  <property>
                      <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
                      <value> org.apache.hadoop.mapred.ShuffleHandler</value>
                  </property>
          </configuration>

$ sudo cp mapred.site.xml.template mapred-site.xml

$ sudo gedit mapred-site.xml

          <configuration>
                  <property>
                      <name>mapreduce.framework.name</name>
                      <value>yarn</value>
                  </property>
          </configuration>

$ sudo gedit hdfs-site.xml

          <configuration>
                  <property>
                      <name>dfs.replication</name>
                      <value>1</value>
                  </property>
                  <property>
                      <name>dfs.namenode.name.dir</name>
                      <value>file:/usr/local/hadoop/hadoop_data/hdfs/namenode</value>
                  </property>
                  <property>
                      <name>dfs.datanode.data.dir</name>
                      <value>file:/usr/local/hadoop/hadoop_store/hdfs/datanode</value>
                  </property>
          </configuration>

15.Create directories for namenode and datanode 

$ cd

$ mkdir -p /usr/local/hadoop/hadoop_data/hdfs/namenode

$ mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode

$ sudo chown chaalpritam:chaalpritam -R /usr/local/hadoop
16.Format namenode by hdfs command

$ hdfs namenode –format

17.Start all the services

$ start-all.sh

This is yarn version of hadoop so there will be services as mentioned below:

$ jps

Namenode
Datanode
Secondary namenode
Node manager
Resource manager
jps

check in browser for specific instances:

http://localhost:8088/ ---> application interface
http://localhost:50070/ ---> overview interface
http://localhost:50090/
http://localhost:50075/ ---> datanode interface

